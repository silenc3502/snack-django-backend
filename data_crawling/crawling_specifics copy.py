from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import time
import csv
import requests
from datetime import datetime

# í¬ë¡¬ ë“œë¼ì´ë²„ ì„¤ì •
service = Service("/opt/homebrew/bin/chromedriver")
options = webdriver.ChromeOptions()
options.add_argument("--no-sandbox")
options.add_argument("--disable-dev-shm-usage")

# ì¹´ì¹´ì˜¤ ì£¼ì†Œ ê²€ìƒ‰ API í‚¤ (í•„ìˆ˜ ì…ë ¥)
KAKAO_API_KEY = '6d9dc3df95f90cbe474e8b518e13f2f2'

# ì„œìš¸ì‹œ êµ¬ ë¦¬ìŠ¤íŠ¸
seoul_gu_list = [
    "ê°•ë‚¨êµ¬", "ê°•ë™êµ¬", "ê°•ë¶êµ¬", "ê°•ì„œêµ¬", "ê´€ì•…êµ¬", "ê´‘ì§„êµ¬", "êµ¬ë¡œêµ¬", "ê¸ˆì²œêµ¬",
    "ë…¸ì›êµ¬", "ë„ë´‰êµ¬", "ë™ëŒ€ë¬¸êµ¬", "ë™ì‘êµ¬", "ë§ˆí¬êµ¬", "ì„œëŒ€ë¬¸êµ¬", "ì„œì´ˆêµ¬", "ì„±ë™êµ¬",
    "ì„±ë¶êµ¬", "ì†¡íŒŒêµ¬", "ì–‘ì²œêµ¬", "ì˜ë“±í¬êµ¬", "ìš©ì‚°êµ¬", "ì€í‰êµ¬", "ì¢…ë¡œêµ¬", "ì¤‘êµ¬", "ì¤‘ë‘êµ¬"
]

# ì¢Œí‘œ ë³€í™˜ í•¨ìˆ˜
def get_lat_lon(address):
    url = "https://dapi.kakao.com/v2/local/search/address.json"
    headers = {"Authorization": f"KakaoAK {KAKAO_API_KEY}"}
    params = {"query": address}
    response = requests.get(url, headers=headers, params=params)

    if response.status_code == 200:
        result = response.json().get('documents', [])
        if result:
            return result[0]['y'], result[0]['x']
    return None, None

# í¬ë¡¬ ë“œë¼ì´ë²„ ì´ˆê¸°í™”
def init_driver():
    driver = webdriver.Chrome(service=service, options=options)
    driver.get('https://map.kakao.com/')
    time.sleep(2)
    return driver

# ê²€ìƒ‰ì–´ ì…ë ¥ ë° ê²€ìƒ‰ ì‹¤í–‰
def search_keyword(driver, keyword):
    search_area = driver.find_element(By.ID, 'search.keyword.query')
    search_area.clear()
    search_area.send_keys(keyword)
    search_area.send_keys(Keys.ENTER)
    time.sleep(3)

    remove_dimmed_layer(driver)
    click_place_tab(driver)
    click_place_more(driver)

# íŒì—… ì œê±°
def remove_dimmed_layer(driver):
    try:
        dimmed_layer = driver.find_element(By.ID, 'dimmedLayer')
        driver.execute_script("arguments[0].style.display='none';", dimmed_layer)
    except:
        pass

# ì¥ì†Œ íƒ­ í´ë¦­
def click_place_tab(driver):
    driver.find_element(By.XPATH, '//*[@id="info.main.options"]/li[2]/a').click()
    time.sleep(2)

# ì¥ì†Œ ë”ë³´ê¸° í´ë¦­
def click_place_more(driver):
    try:
        driver.find_element(By.ID, 'info.search.place.more').click()
        time.sleep(2)
    except:
        pass

# ë©”ë‰´ íƒ­ í¼ì¹˜ê³  ë©”ë‰´ ìˆ˜ì§‘
def expand_menu_tab_and_collect(driver):
    menu_items = []
    try:
        menu_tab = driver.find_element(By.CSS_SELECTOR, 'a[href="#menuInfo"]')
        menu_tab.click()
        time.sleep(2)

        while True:
            try:
                more_button = driver.find_element(By.CSS_SELECTOR, '.wrap_more a.link_more')
                if more_button.is_displayed():
                    more_button.click()
                    time.sleep(2)
                else:
                    break
            except:
                break

        menu_elements = driver.find_elements(By.CSS_SELECTOR, '.list_goods > li')
        for element in menu_elements:
            name = element.find_element(By.CSS_SELECTOR, '.tit_item').text.strip()
            try:
                price = element.find_element(By.CSS_SELECTOR, '.desc_item').text.strip()
            except:
                price = 'ê°€ê²©ì •ë³´ ì—†ìŒ'
            menu_items.append(f'{name} ({price})')

    except Exception as e:
        print(f"âŒ ë©”ë‰´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return ['ë©”ë‰´ ì—†ìŒ']

    return menu_items if menu_items else ['ë©”ë‰´ ì—†ìŒ']

# ë§¤ì¥ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
def get_store_details(driver, detail_url):
    original_window = driver.current_window_handle

    driver.execute_script("window.open(arguments[0]);", detail_url)
    WebDriverWait(driver, 10).until(lambda d: len(d.window_handles) > 1)

    driver.switch_to.window(driver.window_handles[-1])
    time.sleep(2)

    menu_list = expand_menu_tab_and_collect(driver)
    menu_text = ', '.join(menu_list)

    driver.close()
    driver.switch_to.window(original_window)

    return menu_text

# ëª¨ë“  í˜ì´ì§€ í¬ë¡¤ë§
def crawl_all_pages(driver):
    all_data = []

    def process_current_page():
        nonlocal all_data
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        stores = soup.select('.placelist > .PlaceItem')

        for store in stores:
            try:
                name = store.select_one('.head_item .tit_name .link_name').text.strip()
                degree = store.select_one('.rating .score .num').text.strip()
                review_count = store.select_one('.review em[data-id="numberofreview"]').text.strip() or '0'
                address = store.select_one('.info_item .addr').text.strip()
                tel = store.select_one('.info_item .phone').text.strip() if store.select_one('.info_item .phone') else 'ì „í™”ë²ˆí˜¸ ì—†ìŒ'
                detail_url = store.select_one('.contact .moreview')['href']

                menu_text = get_store_details(driver, detail_url)

                print(f"ğŸ“ {name} | í‰ì : {degree} | ë¦¬ë·° {review_count}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
                all_data.append([name, degree, review_count, address, tel, menu_text])

            except Exception as e:
                print(f"âŒ ë§¤ì¥ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

    while True:
        process_current_page()

        try:
            next_button = driver.find_element(By.ID, 'info.search.page.next')
            if "disabled" in next_button.get_attribute("class"):
                print("âœ… ë§ˆì§€ë§‰ í˜ì´ì§€ ë„ë‹¬, í¬ë¡¤ë§ ì¢…ë£Œ")
                break
            next_button.click()
            time.sleep(2)
        except:
            print("âŒ ë‹¤ìŒ ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨, í¬ë¡¤ë§ ì¢…ë£Œ")
            break

    return all_data

# CSV ì €ì¥ (ë‚ ì§œ í¬í•¨ & ìœ„ë„/ê²½ë„ ì¶”ê°€)
def save_to_csv(gu_name, data):
    today = datetime.now().strftime("%Y%m%d")
    filename = f'{today}_{gu_name}_ë§›ì§‘_í¬ë¡¤ë§.csv'

    with open(filename, 'w', encoding='utf-8-sig', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['ì´ë¦„', 'í‰ì ', 'ë¦¬ë·°ìˆ˜', 'ì£¼ì†Œ', 'ìœ„ë„', 'ê²½ë„', 'ì „í™”ë²ˆí˜¸', 'ë©”ë‰´'])

        for row in data:
            name, degree, review_count, address, tel, menu_text = row
            lat, lon = get_lat_lon(address)
            writer.writerow([name, degree, review_count, address, lat, lon, tel, menu_text])

    print(f"âœ… {gu_name} ì €ì¥ ì™„ë£Œ ({filename})")

# ì„œìš¸ì‹œ êµ¬ë³„ í¬ë¡¤ë§ ì‹¤í–‰
def crawl_seoul_gu():
    for gu in seoul_gu_list:
        print(f"ğŸ”¹ {gu} í¬ë¡¤ë§ ì‹œì‘!")
        driver = init_driver()
        search_keyword(driver, f'{gu} ë§›ì§‘')

        all_data = crawl_all_pages(driver)
        save_to_csv(gu, all_data)

        driver.quit()
        print(f"âœ… {gu} í¬ë¡¤ë§ ë° ì €ì¥ ì™„ë£Œ\n")

# ë©”ì¸ ì‹¤í–‰
if __name__ == '__main__':
    crawl_seoul_gu()
