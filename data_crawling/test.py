import requests
import re
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from bs4 import BeautifulSoup
import time
import csv
from datetime import datetime

# âœ… ë°˜ë“œì‹œ ë„¤ APIí‚¤ë¡œ ë³€ê²½í•´ì•¼ í•¨
KAKAO_API_KEY = "ë„ˆì˜_ì¹´ì¹´ì˜¤_API_í‚¤"

service = Service("/opt/homebrew/bin/chromedriver")
options = webdriver.ChromeOptions()
options.add_argument("--no-sandbox")
options.add_argument("--disable-dev-shm-usage")

seoul_gu_list = [
    "ê°•ë‚¨êµ¬", "ê°•ë™êµ¬", "ê°•ë¶êµ¬", "ê°•ì„œêµ¬", "ê´€ì•…êµ¬", "ê´‘ì§„êµ¬", "êµ¬ë¡œêµ¬", "ê¸ˆì²œêµ¬",
    "ë…¸ì›êµ¬", "ë„ë´‰êµ¬", "ë™ëŒ€ë¬¸êµ¬", "ë™ì‘êµ¬", "ë§ˆí¬êµ¬", "ì„œëŒ€ë¬¸êµ¬", "ì„œì´ˆêµ¬", "ì„±ë™êµ¬",
    "ì„±ë¶êµ¬", "ì†¡íŒŒêµ¬", "ì–‘ì²œêµ¬", "ì˜ë“±í¬êµ¬", "ìš©ì‚°êµ¬", "ì€í‰êµ¬", "ì¢…ë¡œêµ¬", "ì¤‘êµ¬", "ì¤‘ë‘êµ¬"
]

def clean_address(address):
    address = re.sub(r"\sMì¸µ", " 1ì¸µ", address)
    address = re.sub(r"\s*\d+[-,~]?\s*\d*\s*ì¸µ", " 1ì¸µ", address)
    address = re.sub(r"\s\d+[-,~]?\d*\s?í˜¸", " 101í˜¸", address)
    address = re.sub(r"\s\(.+?\)", "", address)
    address = re.sub(r"\s[ê°€-í£A-Za-z0-9]+ë¹Œë”©", "", address)
    address = re.sub(r"\s[ê°€-í£A-Za-z0-9]+ì„¼í„°", "", address)
    address = re.sub(r"\s[ê°€-í£A-Za-z0-9]+í˜¸í…”", "", address)
    return address.strip()

def get_coordinates(address):
    cleaned_address = clean_address(address)
    url = "https://dapi.kakao.com/v2/local/search/address.json"
    headers = {"Authorization": f"KakaoAK {KAKAO_API_KEY}"}
    params = {"query": cleaned_address}

    response = requests.get(url, headers=headers, params=params)
    data = response.json()

    if response.status_code == 200 and data["documents"]:
        lat = data["documents"][0]["y"]
        lon = data["documents"][0]["x"]
        return lat, lon

    params = {"query": address}  # ì›ë˜ ì£¼ì†Œë¡œ ì¬ì‹œë„
    response = requests.get(url, headers=headers, params=params)
    data = response.json()

    if response.status_code == 200 and data["documents"]:
        lat = data["documents"][0]["y"]
        lon = data["documents"][0]["x"]
        return lat, lon

    print(f"âŒ ì£¼ì†Œ ë³€í™˜ ì‹¤íŒ¨: {address}")
    return None, None

def init_driver():
    driver = webdriver.Chrome(service=service, options=options)
    driver.get('https://map.kakao.com/')
    time.sleep(2)
    return driver

def search_keyword(driver, keyword):
    search_area = driver.find_element(By.ID, 'search.keyword.query')
    search_area.clear()
    search_area.send_keys(keyword)
    search_area.send_keys(Keys.ENTER)
    time.sleep(3)

    try:
        driver.find_element(By.XPATH, '//*[@id="info.main.options"]/li[2]/a').click()
        time.sleep(2)
    except:
        pass

    try:
        driver.find_element(By.ID, 'info.search.place.more').click()
        time.sleep(2)
    except:
        pass

def expand_menu_tab_and_collect(driver):
    menu_items = []
    try:
        menu_tab = driver.find_element(By.CSS_SELECTOR, 'a[href="#menuInfo"]')
        menu_tab.click()
        time.sleep(2)

        while True:
            try:
                more_button = driver.find_element(By.CSS_SELECTOR, '.wrap_more a.link_more')
                if more_button.is_displayed():
                    more_button.click()
                    time.sleep(2)
                else:
                    break
            except:
                break

        menu_elements = driver.find_elements(By.CSS_SELECTOR, '.list_goods > li')
        for element in menu_elements:
            name = element.find_element(By.CSS_SELECTOR, '.tit_item').text.strip()
            try:
                price = element.find_element(By.CSS_SELECTOR, '.desc_item').text.strip()
            except:
                price = 'ê°€ê²©ì •ë³´ ì—†ìŒ'
            menu_items.append(f'{name} ({price})')
    except:
        return ['ë©”ë‰´ ì—†ìŒ']
    return menu_items if menu_items else ['ë©”ë‰´ ì—†ìŒ']

def get_store_details(driver, detail_url):
    original_window = driver.current_window_handle
    driver.execute_script("window.open(arguments[0]);", detail_url)
    WebDriverWait(driver, 10).until(lambda d: len(d.window_handles) > 1)

    driver.switch_to.window(driver.window_handles[-1])
    time.sleep(2)

    menu_list = expand_menu_tab_and_collect(driver)
    menu_text = ', '.join(menu_list)

    driver.close()
    driver.switch_to.window(original_window)
    return menu_text

def crawl_all_pages(driver):
    all_data = []

    def process_current_page():
        nonlocal all_data
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        stores = soup.select('.placelist > .PlaceItem')

        for store in stores:
            try:
                name = store.select_one('.head_item .tit_name .link_name').text.strip()
                degree = store.select_one('.rating .score .num').text.strip()
                review_count = store.select_one('.review em[data-id="numberofreview"]').text.strip() or '0'
                address = store.select_one('.info_item .addr').text.strip()
                tel = store.select_one('.info_item .phone').text.strip() if store.select_one('.info_item .phone') else 'ì „í™”ë²ˆí˜¸ ì—†ìŒ'
                detail_url = store.select_one('.contact .moreview')['href']

                menu_text = get_store_details(driver, detail_url)

                lat, lon = get_coordinates(address)

                print(f"ğŸ“ {name} | í‰ì : {degree} | ë¦¬ë·° {review_count}ê°œ | ìœ„ë„: {lat} | ê²½ë„: {lon}")
                all_data.append([name, degree, review_count, address, tel, menu_text, lat, lon])

            except Exception as e:
                print(f"âŒ ë§¤ì¥ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

    while True:
        process_current_page()
        try:
            next_button = driver.find_element(By.ID, 'info.search.page.next')
            if "disabled" in next_button.get_attribute("class"):
                break
            next_button.click()
            time.sleep(2)
        except:
            break

    return all_data

def save_to_csv(gu_name, data):
    today = datetime.now().strftime("%Y%m%d")
    filename = f'{today}_{gu_name}_ë§›ì§‘_í¬ë¡¤ë§.csv'
    with open(filename, 'w', encoding='utf-8-sig', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['ì´ë¦„', 'í‰ì ', 'ë¦¬ë·°ìˆ˜', 'ì£¼ì†Œ', 'ì „í™”ë²ˆí˜¸', 'ë©”ë‰´', 'ìœ„ë„', 'ê²½ë„'])
        writer.writerows(data)
    print(f"âœ… {gu_name} ì €ì¥ ì™„ë£Œ ({filename})")

def crawl_seoul_gu():
    for gu in seoul_gu_list:
        print(f"ğŸ”¹ {gu} í¬ë¡¤ë§ ì‹œì‘!")
        driver = init_driver()
        search_keyword(driver, f'{gu} ë§›ì§‘')

        all_data = crawl_all_pages(driver)
        save_to_csv(gu, all_data)

        driver.quit()
        print(f"âœ… {gu} í¬ë¡¤ë§ ë° ì €ì¥ ì™„ë£Œ\n")

if __name__ == '__main__':
    crawl_seoul_gu()
