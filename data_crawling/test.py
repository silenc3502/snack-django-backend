import time
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# í¬ë¡¬ ë“œë¼ì´ë²„ ê²½ë¡œ
CHROME_DRIVER_PATH = "/path/to/your/chromedriver"  # ë³¸ì¸ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •

# WebDriver ì˜µì…˜ ì„¤ì •
options = webdriver.ChromeOptions()
options.add_argument("--headless=new")  # í•„ìš”ì‹œ headless ëª¨ë“œ
options.add_argument("--no-sandbox")
options.add_argument("--disable-dev-shm-usage")

service = Service(CHROME_DRIVER_PATH)
driver = webdriver.Chrome(service=service, options=options)

# í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸° í•¨ìˆ˜
def wait_for_element(driver, by, value, timeout=10):
    try:
        return WebDriverWait(driver, timeout).until(EC.presence_of_element_located((by, value)))
    except:
        return None

# ì•ˆì „í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜
def safe_get_text(element):
    return element.text.strip() if element else ""

# ë§¤ì¥ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§ í•¨ìˆ˜
def crawl_store_info(store_url):
    driver.get(store_url)
    time.sleep(2)

    if not driver.current_url.startswith("https://place.map.kakao.com/"):
        print(f"âŒ ì˜ëª»ëœ ìƒì„¸í˜ì´ì§€: {driver.current_url}")
        return None

    store_name_element = wait_for_element(driver, By.CSS_SELECTOR, "h3.tit_place", timeout=5)
    if store_name_element is None:
        print(f"âŒ ìƒì„¸í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: {store_url}")
        return None

    store_info = {
        "ìƒí˜¸ëª…": safe_get_text(store_name_element),
        "ì¹´í…Œê³ ë¦¬": safe_get_text(driver.find_element(By.CSS_SELECTOR, ".info_cate")),
        "ì£¼ì†Œ": safe_get_text(driver.find_element(By.CSS_SELECTOR, ".txt_detail")),
        "ì „í™”ë²ˆí˜¸": safe_get_text(driver.find_element(By.CSS_SELECTOR, ".info_suggest .txt_detail")),
        "ë³„ì ": safe_get_text(driver.find_element(By.CSS_SELECTOR, ".num_star")),
        "ë¦¬ë·°ìˆ˜": safe_get_text(driver.find_element(By.CSS_SELECTOR, ".link_review .info_num")),
        "ì˜ì—…ì‹œê°„": safe_get_text(driver.find_element(By.CSS_SELECTOR, ".info_runtime")),
        "íœ´ë¬´ì¼": "",  # í•„ìš”ì‹œ ì¶”ê°€ í¬ë¡¤ë§ ê°€ëŠ¥
        "ìœ„ë„": driver.execute_script("return mapview.map.getCenter().getLat();"),
        "ê²½ë„": driver.execute_script("return mapview.map.getCenter().getLng();")
    }

    # ë©”ë‰´ í¬ë¡¤ë§
    store_info["ë©”ë‰´"] = []
    try:
        menu_elements = driver.find_elements(By.CSS_SELECTOR, ".list_goods .info_goods")
        for menu in menu_elements:
            name = safe_get_text(menu.find_element(By.CSS_SELECTOR, ".tit_item"))
            price = safe_get_text(menu.find_element(By.CSS_SELECTOR, ".desc_item"))
            store_info["ë©”ë‰´"].append({"ë©”ë‰´ëª…": name, "ê°€ê²©": price})
    except:
        pass

    return store_info

# êµ¬ë³„ ì¥ì†Œ ëª©ë¡ ìˆ˜ì§‘ í•¨ìˆ˜
def get_places_by_district(district):
    search_url = f"https://map.kakao.com/?q={district}+ë§›ì§‘"
    driver.get(search_url)
    time.sleep(2)

    place_links = []
    for _ in range(3):  # ì²« 3í˜ì´ì§€ íƒìƒ‰
        places = driver.find_elements(By.CSS_SELECTOR, ".link_name")
        place_links.extend([p.get_attribute("href") for p in places])
        
        next_btn = driver.find_element(By.CSS_SELECTOR, ".btn_next")
        if "off" in next_btn.get_attribute("class"):
            break
        next_btn.click()
        time.sleep(2)

    return place_links

# ì„œìš¸ì‹œ ì „ì²´ êµ¬ ë¦¬ìŠ¤íŠ¸
seoul_districts = [
    "ê°•ë‚¨êµ¬", "ê°•ë™êµ¬", "ê°•ë¶êµ¬", "ê°•ì„œêµ¬", "ê´€ì•…êµ¬", "ê´‘ì§„êµ¬",
    "êµ¬ë¡œêµ¬", "ê¸ˆì²œêµ¬", "ë…¸ì›êµ¬", "ë„ë´‰êµ¬", "ë™ëŒ€ë¬¸êµ¬", "ë™ì‘êµ¬",
    "ë§ˆí¬êµ¬", "ì„œëŒ€ë¬¸êµ¬", "ì„œì´ˆêµ¬", "ì„±ë™êµ¬", "ì„±ë¶êµ¬", "ì†¡íŒŒêµ¬",
    "ì–‘ì²œêµ¬", "ì˜ë“±í¬êµ¬", "ìš©ì‚°êµ¬", "ì€í‰êµ¬", "ì¢…ë¡œêµ¬", "ì¤‘êµ¬", "ì¤‘ë‘êµ¬"
]

# ìµœì¢… ì‹¤í–‰ í•¨ìˆ˜
def main():
    all_data = []
    for district in seoul_districts:
        print(f"ğŸ“ {district} ë§›ì§‘ í¬ë¡¤ë§ ì‹œì‘...")
        place_urls = get_places_by_district(district)

        for url in place_urls:
            info = crawl_store_info(url)
            if info:
                info["êµ¬"] = district
                all_data.append(info)

    df = pd.DataFrame(all_data)
    df.to_excel("ì„œìš¸_ë§›ì§‘_ì •ë³´_í¬ë¡¤ë§_ê²°ê³¼.xlsx", index=False)
    print("âœ… í¬ë¡¤ë§ ì™„ë£Œ ë° ì €ì¥ ì™„ë£Œ")

if __name__ == "__main__":
    main()
    driver.quit()
